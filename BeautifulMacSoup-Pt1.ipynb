{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b243e7c-64db-4abd-991b-200d84617f32",
   "metadata": {},
   "source": [
    "# <font color='grey'>Web Scraping with Python and Beautiful Soup Pt I:<br>Scraping HTML</font>\n",
    "\n",
    "## <font color='orange'>Workshop Description</font>\n",
    "\n",
    "This workshop will introduce students to techniques for scraping information from the web using Python’s Beautiful Soup (bs4) toolkit. We will begin with a basic overview of the “anatomy” or structure of a webpage. Students will then learn how to write a script for extracting textual data from websites like Reddit and organizing it into spreadsheets. The second half of the workshop will explore how to use Python Pandas library to clean and analyze your data. In addition to technical skills, participants are encouraged to engage with critical questions like: What can we, as researchers, learn from publicly available data? As well as, what are the potential ethical and legal complexities around data harvesting, and how do we do it responsibly?\n",
    "\n",
    "By the end of the workshop, students will know:\n",
    "\n",
    "- Scrape HTML content from a webpage\n",
    "- Clean and analyze data\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "*Tip Use the Anaconda package manager to install Jupyter, Python 3, and Beautiful Soup: [Installation Guide](https://docs.anaconda.com/free/anaconda/install/)*\n",
    "\n",
    "- Install Jupyter Notebook: [Installation Guide](https://jupyter.org/install)\n",
    "    - [JupyterLite](https://jupyterlite.readthedocs.io/) is a browser-based version that comes with pre-installed packages, no installation required. \n",
    "- Install Python 3 \n",
    "- Install Beautiful Soup 4: See [Installation Guide](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-beautiful-soup)\n",
    "\n",
    "### Resources:\n",
    "- [BeautifulSoup4 Quickstart](https://beautiful-soup-4.readthedocs.io/en/latest/#quick-start)\n",
    "- [Markdown Cheat Sheet](https://www.markdownguide.org/cheat-sheet/)\n",
    "- [Regex 101](https://regex101.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c31da50-7d22-4a15-8860-b05ccac938db",
   "metadata": {},
   "source": [
    "## <font color='orange'>What is Jupyter Notebook?</font> \n",
    "\n",
    "- Jupyter Notebook is a Graphic User Interface (GUI) that sits on top of the code, making it easier to interact with.\n",
    "- Jupyter Notebook is for:\n",
    "    - a) writing and executing \"live\" \"live\" computer code\n",
    "    - b) creating written and visual notes or commentary (like for tutorials!)\n",
    "\n",
    "### Using Jupyter:\n",
    "- **Cells** are boxes for entering code or text.\n",
    "- Switch between **Cell Types** using the dropdown menu:\n",
    "    - a) **Code**: e.g. Python, Java, R\n",
    "    - b) **Markdown**: create text and visual content, easy-to-read and write\n",
    "    \n",
    "### Working with Cells\n",
    "- **Double-click** a Cell to edit it.\n",
    "- You can **Insert** Cells above or below, **Copy and Paste** Cells contents, **Move** Cells up/down, and **Delete** Cells.\n",
    "- Click **Shift + Return** to execute Cell contents (run code).\n",
    "- Click the **STOP** icon to stop (interrupt) code.\n",
    "\n",
    "### <font color='grey'> *Try it!*</font>\n",
    "- In the upper-right corner of *this* cell, click the **Rectangle Box with a Plus Sign Underneath** to add a cell below.\n",
    "- Click on the cell.\n",
    "- At the top, change the dropdown menu to select \"Code.\"\n",
    "- Type the code below\n",
    "\n",
    "```\n",
    "print('Oh, what a beautiful morning!')\n",
    "```\n",
    "\n",
    "- Click ** **Shift + Return**\n",
    "- If successful, you'll see the lyrics to the opening song from Oklahoma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "66c396b5-005c-4d5e-a7c8-764db0b1bebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, what a beautiful morning!\n"
     ]
    }
   ],
   "source": [
    "print('Oh, what a beautiful morning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc0264-7def-4980-868b-a28647c67930",
   "metadata": {},
   "source": [
    "## <font color='orange'>What is Web Scraping?</font>\n",
    "- Extracting content (text and/or metadata) from websites\n",
    "- Can iterate over multiple websites/pages\n",
    "\n",
    "### Static vs Dynamic Websites\n",
    "- **Static:** flat, pure HTML\n",
    "    - what you see is what you get!\n",
    "- **Dynamic:** database-driven, often relies on Javascipt\n",
    "    - generates content on the fly, often personalised (e.g. clicking, scrolling)\n",
    " \n",
    "*Tip: Not sure if a website is dynamic? Try disabling Javascript in Chrome's Dev Tools. On a webpage, right-click \"Inspect\" > Open Command Menu (click three dots) > Select \"Run Command\" > begin typing Javascript > Select \"Disable Javascript\".*\n",
    "\n",
    "### Techniques: HTML vs APIs\n",
    "- ### HTML Scraping (with Beautiful Soup):\n",
    "    - DIY approach\n",
    "    - Works for simple, HTML\n",
    "- ### Application Programming Interfaces (APIs):\n",
    "  - Mediates between two systems (like a mobile app and your phone)\n",
    "  - Allows to exhcange data\n",
    "  - Major social media platforms offer APIs to developers, as a way to encourage the creation of third-party apps and services.\n",
    "  - ... but APIs also restrict usage in different ways (more on that in Part Two)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3dfc64-d56b-4e0c-9acf-bebe6fab72bc",
   "metadata": {},
   "source": [
    "## <font color='orange'>Web Scraping Etiquette</font>\n",
    "\n",
    "### Best Practices\n",
    "- Take only what you need\n",
    "- Anonymise or (better yet!) avoid scraping identifying information\n",
    "- Try not to overload with requests; build in pauses (e.g. sleep)\n",
    "- Identify yourself\n",
    "- Credit source\n",
    "\n",
    "### Robots.txt\n",
    "- Guidelines for webscrapers about which parts of a site to scrape\n",
    "- Helps prevent overloading\n",
    "- Plain text file, found in root directory\n",
    "- For example, here is a directive from the [McMaster University website](https://www.mcmaster.ca/robots.txt), which bans all webcrawlers from scraping content from pages in the \"busstrike\" filepath: \n",
    "\n",
    "```\n",
    "    User-agent: *\n",
    "    Disallow:   /busstrike/\n",
    "```\n",
    "\n",
    "- Robots.txt files *can* negatively impact SEO (findability) if used incorrectly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3857ad-ceaa-4b52-a331-7b707e31a6f3",
   "metadata": {},
   "source": [
    "## <font color='orange'> Check for Permission to Scrape </font>\n",
    "- You can use Python tools like **RobotFileParser** to check whether you're allowed to fetch the contents of the particular website.\n",
    "- Today we'll be scraping rebuttal speeches from USCB's [American Presidency Project](https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union/list) website.\n",
    "- The code below queries the robot.txt protocols for specific sites.\n",
    "\n",
    "### <font color='grey'>Let's try it!</font>\n",
    "- Click *Shift + Return* to execute the code.\n",
    "- To try checking a different website's robot.txt protocols, just replace the URL.\n",
    "\n",
    "*See [RobotFileParser Documentation](https://docs.python.org/3/library/urllib.robotparser.html) to learn more about how this tool works.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3d6f3da5-364e-4f91-afaf-47bccc8399ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching is allowed.\n"
     ]
    }
   ],
   "source": [
    "import urllib.robotparser\n",
    "\n",
    "# Create instance of RobotFileParser\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "\n",
    "# Specify the URL of the site's robots.txt file\n",
    "rp.set_url(\"https://www.presidency.ucsb.edu/robots.txt\")\n",
    "rp.read()\n",
    "\n",
    "# Specify the user agent and URL you're interested in\n",
    "user_agent = 'MyWebScraper'\n",
    "url_to_scrape = \"https://www.presidency.ucsb.edu/documents/republican-party-response-president-obamas-address-before-joint-session-the-congress-the-3\"\n",
    "\n",
    "# Check if fetching the URL is allowed for your user agent\n",
    "can_fetch = rp.can_fetch(user_agent, url_to_scrape)\n",
    "\n",
    "# Check's request rate (number of requests and seconds between)\n",
    "rrate = rp.request_rate(\"*\")\n",
    "\n",
    "if can_fetch:\n",
    "    print(\"Fetching is allowed.\")\n",
    "else:\n",
    "    print(\"Fetching is disallowed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0b5b6-0680-49f1-a339-1e136368c5e8",
   "metadata": {},
   "source": [
    "## <font color='orange'>About Beautiful Soup</font>\n",
    "\n",
    "### What is Beautitful Soup?\n",
    "- A Python library for parsing (i.e. extracting *specific* content) from HTML and XML documents\n",
    "\n",
    "### Installation Instructions\n",
    "- To begin, first you need to install Beautiful Soup 4 and its dependencies, mainly Python3.\n",
    "- Installation instructions can be found under **'Requirements'** section of the **Workshop Description.**\n",
    "\n",
    "## <font color='orange'>Making the Soup</font>\n",
    "1. Import bs4 and urrllib (another python library for opening URLs).\n",
    "2. Fetch the website content using a urrllib lib \"request\" and \"open\"\n",
    "3. Use Beautiful Soup to extract the HTML.\n",
    "4. Print a 'prettified' or reader friendly version of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "156d76e1-6f06-4649-acce-644d6632de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "r = urllib.request.urlopen('https://www.presidency.ucsb.edu/documents/republican-party-response-president-bidens-address-before-joint-session-the-congress-the-1').read()\n",
    "\n",
    "soup = BeautifulSoup(r, 'html.parser')\n",
    "\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36d30e-ceb5-4a69-9f87-f6b005a8cd88",
   "metadata": {},
   "source": [
    "## <font color='orange'>Anatomy of a Website</font>\n",
    "- HTML websites are made up of different parts (e.g. head, body, paragraphs, tables)\n",
    "- You can demarcate different sections (elements) using *div tags*\n",
    "- Divs can be further classified into *classes* and *IDs*\n",
    "\n",
    "## <font color='orange'>Examine the Website</font>\n",
    "1. On the webpage you want to examine, *right-click* and select *Inspect*\n",
    "2. Under 'Elements,' hover over different parts of the website to see how different elements are tagged and fit within the overall structure of the website.\n",
    "\n",
    "## <font color='orange'>Find our Ingredients</font>\n",
    "- Once you've made the soup, you can hone in on specific elements.\n",
    "\n",
    "  ### <font color='grey'>Try it!</font>\n",
    "  - Return the *title* of the webpage:\n",
    "      ```\n",
    "      soup.title\n",
    "      ```\n",
    "  \n",
    "  - Return the *first paragraph*:\n",
    "      ```\n",
    "      soup.p\n",
    "      ```\n",
    "        \n",
    "  - Return the *first link*:\n",
    "    ```\n",
    "    soup.a\n",
    "    ```\n",
    "        \n",
    "  - Return *all links*:\n",
    "        \n",
    "    ```\n",
    "    soup.find_all('a')\n",
    "    ```\n",
    "    \n",
    "3. We want to grab only the speech text, which is in the div class \"field-docs-content.\"\n",
    "- To return the contents of this div, use the code below.\n",
    "- *Note: We will create a new object from this text by assigning it the name \"speech\" using the equals sign.\n",
    "\n",
    "```\n",
    "speech = soup.find(\"div\", {\"class\": \"field-docs-content\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "47dd9596-0e48-4e72-bb5b-0a520e288871",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = soup.find(\"div\", {\"class\": \"field-docs-content\"})\n",
    "#print(speech.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de01d6-04dd-41d1-a25f-a4fadab404ca",
   "metadata": {},
   "source": [
    "### Continue to Refine\n",
    "- But wait...!\n",
    "- Before the start of the transcript, there is an extra chunk of contextual information.\n",
    "- Let's exclude this paragraph...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8eff5958-844e-4190-a7ef-cbf35c406fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_wintro = soup.find(\"div\", {\"class\": \"field-docs-content\"})\n",
    "just_speech = speech_wintro.find_all('p')[1:]\n",
    "#print(just_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584cf8c-9fc3-4f5d-a2d2-377ef382ca04",
   "metadata": {},
   "source": [
    "- Let's also extract *just the text* and get rid of the \"p\" tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fc3861d8-a6aa-492d-adea-6cafca8f3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = ' '.join([tag.get_text() for tag in just_speech])\n",
    "#print(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572a71c-a4eb-4247-9845-05013040079e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "aa3a7dc5-a593-4dc1-b7a0-c0f8eca1e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = urllib.request.urlopen('https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union/list').read()\n",
    "soup = BeautifulSoup(r, 'html.parser')\n",
    "#print(main_page_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc47b1-bc78-4466-a0bd-386c5bef2817",
   "metadata": {},
   "source": [
    "## <font color='orange'>Build the Corpus</font>\n",
    "- Grabbing a single chunk of text is easy (why not just copy/paste?) \n",
    "- Web scraping is most useful for building *datasets.*\n",
    "- Let's compile our corpus...  \n",
    "\n",
    "### Iterate List Items\n",
    "- The website has an [index](https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union/list) of all the rebuttal speeches, which includes this information and also links to the individual speech pages from our initial test.\n",
    "\n",
    "1. ### Make New Soup\n",
    "- To extract information from the index page (as opposed to the individual page), we'll need to make a new soup.\n",
    "\n",
    "```\n",
    "r = urllib.request.urlopen('https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union/list').read()\n",
    "#soup = BeautifulSoup(r)\n",
    "soup = BeautifulSoup(r, 'html.parser')\n",
    "```\n",
    "\n",
    "2. ### Inspect Page\n",
    "- After inspecting the page, we can see that the information we needed is stored as rows in a table.\n",
    "- The dates are hyperlinks that link to the speeches.\n",
    "\n",
    "3. ### Narrow Scope\n",
    "- First let's define the section of text (the table rows) that we want to scrape data from.\n",
    "\n",
    "```\n",
    "rows = soup.tbody.find_all('tr')\n",
    "\n",
    "```\n",
    "\n",
    "4. ### Iterate List of Speeches\n",
    "- Then we'll need to cycle through the list of rebuttal speeches, executing the same code each time.\n",
    "\n",
    "```\n",
    "for row in rows:\n",
    "```\n",
    "\n",
    "5. ### Check for link\n",
    "- Before executing the code, we need to check to see if row contains a link.\n",
    "- If so, we'll grab the hyperlink...\n",
    "- ... then we're off to the races!\n",
    "\n",
    "```\n",
    "a = row.find('a')\n",
    "        \n",
    "if a:\n",
    "    URL = a['href']\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c025a-93bf-4444-aa64-df6a8153cbe7",
   "metadata": {},
   "source": [
    "## <font color='orange'>Gather Metadata</font>\n",
    "- But wait!\n",
    "- What other information do we need?\n",
    "\n",
    "1. ### Get Date\n",
    "- The hyperlink is embedded in the speech date.\n",
    "- All we need to do is extract the text from the link.\n",
    "  \n",
    "```\n",
    "date = a.get_text()\n",
    "```\n",
    "\n",
    "2. ### Format Date\n",
    "- However, the date is just plain text \"e.g January 31, 1990\"\n",
    "- We can translate it into a special datetime_object (using the datetime python library), so that our code recognizes this as special \"datetime\" object.\n",
    "- Then we can reformat our dates in any style we like (e.g. 1990-01-31)\n",
    "- Doing so gives us the option to order our list by time, and examien trends (in speech text) overtime.\n",
    "\n",
    "```\n",
    "datetime_object = datetime.strptime(date, '%B %d, %Y')\n",
    "print(datetime_object.date())\n",
    "```\n",
    "\n",
    "3. ### Get Speaker Name\n",
    "- In HTML, table rows are divided into cells (tagged as 'td')\n",
    "- Examining the website shows that the speaker name is in the 3rd cell.\n",
    "- In Python, you can access elements in a list or series using square brackets.\n",
    "- As before, we use the function 'get_text()' to extract just the text.\n",
    "- Finally, we strip extra whitespace from the start and end of the string with 'strip.()'\n",
    "\n",
    "```\n",
    "tds = row.find_all('td')\n",
    "name = tds[2].get_text().strip()\n",
    "print(name)\n",
    "```\n",
    "\n",
    "4. ### Get President's Name\n",
    "- Finally, we grab the name of the sitting President at the time.\n",
    "- The President's name is part of the title, which is enclosed in 'h1' tags.\n",
    "\n",
    "```\n",
    "title = soup.h1.get_text()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da9e7e-75e2-4213-857e-3a1b64dc94e1",
   "metadata": {},
   "source": [
    "## <font color='orange'>Regular Expressions</font>\n",
    "\n",
    "- What if you need to grab a section of text, which *isn't* enclosed in special tags?\n",
    "- Here's where it gets a bit tricky (but doable!)\n",
    "- **Regular Expressions** are tools for matching characters (like find and replace!). It works by using a a combination of brackets and special characters to capture only what you need.\n",
    "- Regular Expressions can be hard to master, but there are online tools (like [RegexR](https://regexr.com/) for testing your code.\n",
    "\n",
    "### Grab *just* the President's Name\n",
    "- Once we've got our regular expression (like a net), we can search for a match using the format below.\n",
    "\n",
    "```\n",
    "\n",
    "president_match = re.search(r'^.*President ([a-zA-Z]*)\\'s', title)\n",
    "president = president_match.group(1) if president_match else 'Unknown'\n",
    "print(president)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603fc722-c3ec-4d4f-aa78-a0b10386d45f",
   "metadata": {},
   "source": [
    "## <font color='orange'>Ethical Practices</font>\n",
    "\n",
    "### Identify yourself as 'friendly'\n",
    "- At the start of your code, set a new variable 'user_agent.'\n",
    "- Add a name for your bot.\n",
    "- Add a link to your website (or institution).\n",
    "\n",
    "```\n",
    "# Identify your bot using a User-Agent string\n",
    "user_agent = 'MyWebScraper/1.0 (+https://github.com/cmiya)'\n",
    "```\n",
    "- In the initial request, insert a new line with a 'header' that identifies yourself to the server.\n",
    "\n",
    "```\n",
    "req = urllib.request.Request(\n",
    "    URL,\n",
    "    headers={\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### Insert Pauses\n",
    "- We don't want to overload the server!\n",
    "- It's good practice to insert \"rests.\"\n",
    "- Here, we've added a 10-15 second pause between each request. \n",
    "\n",
    "```\n",
    "#Give server time to breathe!\n",
    "time.sleep(random.uniform(10, 15))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea77b77-0c6d-41eb-a1d5-85f8eb1a9341",
   "metadata": {},
   "source": [
    "## <font color='orange'>Save Dataset</font>\n",
    "- Finally, let's save our dataset.\n",
    "\n",
    "1. ### Create a Folder and CSV File.\n",
    "- At the *start* of your code, establish where you want to save your data.\n",
    "\n",
    "``` \n",
    "folder_path = 'Desktop/rebuttals'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "csv_path = 'Desktop/USRebuttals.csv'\n",
    "with open(csv_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['year', 'president', 'rebuttal_speaker'])\n",
    "```\n",
    "\n",
    "2. ### Set Filename\n",
    "- As you cycle through the list, for each speech you need to create a new file name and path.\n",
    "- The date is unique to each speech, so we'll use that to title the files.\n",
    "\n",
    "```\n",
    "\n",
    "file_date = datetime_object.date().strftime('%Y-%m-%d')\n",
    "file_name = f'rebut-{file_date}.txt'\n",
    "full_path = os.path.join(folder_path, file_name)\n",
    "```\n",
    "\n",
    "3. ### Save File to Folder\n",
    "- Then we'll save the speech as a txt file to our folder.\n",
    "\n",
    "with open(full_path, 'w') as f:\n",
    "    f.write(speech)\n",
    "\n",
    "3. ### Save Metadata as CSV\n",
    "- Finally, we'll write the date, president name, and speaker name to a new row in our csv spreadshreet.\n",
    "\n",
    "```\n",
    "writer.writerow([datetime_object.date(), president, name])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11aee7-11fd-46c2-9185-2a054c55cf50",
   "metadata": {},
   "source": [
    "## <font color='orange'>Finished Code</font>\n",
    "\n",
    "- Now that we've built the components of our 'bot,' let's put the pieces together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "32a2be80-e52d-4a31-8988-1167f0651eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109261\n",
      "1991-01-29\n",
      "Senator George Mitchell (ME)\n",
      "Bush\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109260\n",
      "1992-01-28\n",
      "House Speaker Tom Foley (WA)\n",
      "Bush\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109235\n",
      "1993-02-17\n",
      "Rep. Bob Michel (IL)\n",
      "Clinton\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109236\n",
      "1994-01-25\n",
      "Senator Robert Dole (KS)\n",
      "Clinton\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109259\n",
      "1995-01-24\n",
      "Governor Christine Todd Whitman (NJ)\n",
      "Clinton\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109237\n",
      "1996-01-23\n",
      "Senator Robert Dole (KS)\n",
      "Clinton\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109238\n",
      "1997-02-04\n",
      "Rep. J.C. Watts (OK)\n",
      "Clinton\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109239\n",
      "1998-01-27\n",
      "Senator Trent Lott (MS)\n",
      "Clinton\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109240\n",
      "1999-01-19\n",
      "Rep. Jennifer Dunn (WA) and Rep. Steven Largent (OK)\n",
      "Clinton\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109241\n",
      "2000-01-27\n",
      "Senator Susan Collins (ME) and Senator William Frist (TN)\n",
      "Clinton\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109244\n",
      "2001-02-27\n",
      "Senator Tom Daschle (SD) and Rep. Richard Gephardt (MO).\n",
      "Bush\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109245\n",
      "2002-01-29\n",
      "Rep. Richard Gephardt (MO)\n",
      "Bush\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109246\n",
      "2003-01-28\n",
      "Governor Gary Locke (WA)\n",
      "Bush\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109247\n",
      "2004-01-20\n",
      "Senator Tom Daschle (SD) and Rep. Nancy Pelosi (CA)\n",
      "Bush\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109248\n",
      "2005-02-02\n",
      "Rep. Nancy Pelosi (CA) and Senator Harry Reid (NV)\n",
      "Bush\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109249\n",
      "2006-01-31\n",
      "Governor Timothy Kaine (VA)\n",
      "Bush\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109250\n",
      "2007-01-23\n",
      "Senator James Webb (VA)\n",
      "Bush\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109251\n",
      "2008-01-28\n",
      "Governor Kathleen Sebelius (KS)\n",
      "Bush\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109252\n",
      "2009-02-24\n",
      "Governor Bobby Jindal (LA)\n",
      "Obama\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109253\n",
      "2010-01-27\n",
      "Governor Bob McDonnell (VA)\n",
      "Obama\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109254\n",
      "2011-01-25\n",
      "Representative Paul D. Ryan (WI)\n",
      "Obama\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109255\n",
      "2012-01-24\n",
      "Governor Mitch Daniels (IN)\n",
      "Obama\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109256\n",
      "2013-02-12\n",
      "Senator Marco Rubio (FL)\n",
      "Obama\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109257\n",
      "2014-01-28\n",
      "Representative Cathy McMorris Rodgers (WA)\n",
      "Obama\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=109258\n",
      "2015-01-20\n",
      "Senator Joni Ernst (IA)\n",
      "Obama\n",
      "https://www.presidency.ucsb.edu/ws/index.php?pid=111392\n",
      "2016-01-12\n",
      "Governor Nikki Haley (SC)\n",
      "Obama\n",
      "https://www.presidency.ucsb.edu/documents/democratic-party-response-president-trumps-address-before-joint-session-the-congress\n",
      "2017-02-28\n",
      "Former Governor Steve Beshear (KY)\n",
      "Trump\n",
      "https://www.presidency.ucsb.edu/documents/democratic-party-response-president-trumps-address-before-joint-session-the-congress-the\n",
      "2018-01-30\n",
      "Representative Joe Kennedy (MA)\n",
      "Trump\n",
      "https://www.presidency.ucsb.edu/documents/democratic-party-response-president-trumps-address-before-joint-session-the-congress-the-0\n",
      "2019-02-05\n",
      "Stacey Abrams (Former Minority Leader, GA House of Representatives and candidate for Governor of Georgia)\n",
      "Trump\n",
      "https://www.presidency.ucsb.edu/documents/democratic-party-response-president-trumps-2020-state-the-union-address\n",
      "2020-02-04\n",
      "Governor Gretchen Whitmer (MI)\n",
      "Rep. Veronica Escobar (TX) Spanish-language response.\n",
      "Trump\n",
      "https://www.presidency.ucsb.edu/documents/republican-party-response-president-bidens-address-before-joint-session-the-congress\n",
      "2021-04-28\n",
      "Senator Tim Scott (SC)\n",
      "Biden\n",
      "https://www.presidency.ucsb.edu/documents/republican-party-response-president-bidens-address-before-joint-session-the-congress-the\n",
      "2022-03-01\n",
      "Governor Kim Reynolds (IA)\n",
      "Biden\n",
      "https://www.presidency.ucsb.edu/documents/republican-party-response-president-bidens-address-before-joint-session-the-congress-the-0\n",
      "2023-02-07\n",
      "Governor Sarah Huckabee Sanders (AR)\n",
      "Biden\n",
      "https://www.presidency.ucsb.edu/documents/republican-party-response-president-bidens-address-before-joint-session-the-congress-the-1\n",
      "2024-03-07\n",
      "Senator Katie Britt (AL)\n",
      "Biden\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "\n",
    "r = urllib.request.urlopen('https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union/list').read()\n",
    "#soup = BeautifulSoup(r)\n",
    "\n",
    "soup = BeautifulSoup(r, 'html.parser')\n",
    "\n",
    "#print(soup.prettify())\n",
    "\n",
    "# Identify your bot using a User-Agent string\n",
    "user_agent = 'MyWebScraper/1.0 (+https://github.com/cmiya)'\n",
    "\n",
    "rows = soup.tbody.find_all('tr')\n",
    "\n",
    "folder_path = 'Desktop/rebuttals'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "csv_path = 'Desktop/USRebuttals.csv'\n",
    "with open(csv_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['year', 'president', 'rebuttal_speaker'])\n",
    "    \n",
    "    for row in rows:\n",
    "        a = row.find('a')\n",
    "        \n",
    "        if a:\n",
    "            URL = a['href']\n",
    "            print(URL)\n",
    "\n",
    "            date = a.get_text()\n",
    "            try:\n",
    "                datetime_object = datetime.strptime(date, '%B %d, %Y')\n",
    "                print(datetime_object.date())\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing date: {e}\")\n",
    "                continue\n",
    "\n",
    "            tds = row.find_all('td')\n",
    "            name = tds[2].get_text().strip()\n",
    "            print(name)\n",
    "\n",
    "            # Create a request with the User-Agent header\n",
    "            req = urllib.request.Request(\n",
    "                URL,\n",
    "                headers={\n",
    "                    'User-Agent': user_agent\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # Use the request object to open the URL\n",
    "                r = urllib.request.urlopen(req).read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error opening URL {URL}: {e}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(r, 'html.parser')\n",
    "\n",
    "            title = soup.h1.get_text()\n",
    "            president_match = re.search(r'^.*President ([a-zA-Z]*)\\'s', title)\n",
    "            president = president_match.group(1) if president_match else 'Unknown'\n",
    "            print(president)\n",
    "\n",
    "            speech_wintro = soup.find(\"div\", {\"class\": \"field-docs-content\"})\n",
    "            just_speech = speech_wintro.find_all('p')[1:]\n",
    "            speech = ' '.join([tag.get_text() for tag in just_speech])\n",
    "\n",
    "            file_date = datetime_object.date().strftime('%Y-%m-%d')\n",
    "            file_name = f'rebut-{file_date}.txt'\n",
    "            full_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            with open(full_path, 'w') as f:\n",
    "                f.write(speech)\n",
    "\n",
    "            writer.writerow([datetime_object.date(), president, name])\n",
    "\n",
    "            #Give server time to breathe!\n",
    "            time.sleep(random.uniform(1, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
